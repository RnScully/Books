{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#from Scraper import gr_db_cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db=client['reviews']\n",
    "coll=db['user_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".find() creates a \"cursor\" which pulls the data. .limit() keeps you from pulling all the data. Wrapping the \"cursor\" in a list comprehension builds that cursor out iteratively into a list containing the things you want to manipulate. Ok? Cool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [x for x in coll.find().limit(1000)]\n",
    "test[0]['userid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, its more complicated than that, because all of this is nested *again* So in the cursor list? you've got a dictonary that holds user_id and a list of reviews. so index into 'reviews' and you get to a list of your actual scraped bits, the data from goodreads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = test[3]['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you're going to want to make those giant strings of html into a handle-able-able object. So put it in a bowl with beatiful soup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-69ab689e2eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'review' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(review, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = soup.find_all(class_ = re.compile('author'))[0].text.strip('author ').split('\\n')[0]\n",
    "author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_type =soup.find_all(class_ = re.compile('format'))[0].text.split('\\n')[1].strip()\n",
    "book_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn = int(soup.find_all(class_ = re.compile('isbn13'))[0].text.strip('isbn13').strip())\n",
    "isbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find_all(class_ = re.compile('title'))[0].text.split('\\n')[1].strip()\n",
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = soup.find_all(class_ = re.compile('title'))[0].text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = int(soup.find_all(class_ =re.compile('num_pages'))[0].text.split()[2])\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_rate = float(soup.find_all(class_ =re.compile('avg_rating'))[0].text.split()[2])\n",
    "av_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rate = int(soup.find_all(class_ =re.compile('num_ratings'))[0].text.split()[2].replace(',',''))\n",
    "num_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_rate(qual_state):\n",
    "    '''\n",
    "    a function that turns goodreads's \"I liked it\" or \"I did not like it\" star categories\n",
    "    into the numerical 1-5 rating that they visually imply. \n",
    "    ++++++\n",
    "    Attributes\n",
    "    qual_state (list) a split string pulled from the beautiful soup output of .text on the rating object\n",
    "    ++++++\n",
    "    Returns\n",
    "    user_rating (int): 1-5 score based on NUMBER OF STARS SELECTED BY THE RATER. I honestly don't understand why that's not the output in the HTML. \n",
    "    '''\n",
    "    if qual_state[-3:] == ['it', 'was', 'amazing']:\n",
    "        user_rating = 5\n",
    "    elif qual_state[-3:] ==['really','liked','it']:\n",
    "        user_rating = 4\n",
    "    elif qual_state[-2:] ==['liked', 'it']:\n",
    "        '''note that I belive any that include \"really\" will be given 4\n",
    "        before we get to this elif statement, therefore we don't need \n",
    "        to worry about the issues of \"really liked it\" and \"liked it\"\n",
    "        overlapping'''\n",
    "        user_rating= 3\n",
    "    elif qual_state[-3:]==['it','was','ok']:\n",
    "        user_rating = 2\n",
    "    elif qual_state[-3:]==['not','like','it']:\n",
    "        user_rating = 1\n",
    "    else:\n",
    "        user_rating = 0\n",
    "    return user_rating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating = str_to_rate(soup.find_all(class_ =re.compile('field rating'))[0].text.split())\n",
    "user_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(find_lim = 10):\n",
    "    '''\n",
    "    a function that reads in goodreads user review tables gathered\n",
    "    by the gr_scraper and returns a list for schema\n",
    "    +++++++++++\n",
    "    Atributes\n",
    "    find_lim (int): how many user-reviews to clean. pass 'all' into find lim\n",
    "                    to clean entire db.   \n",
    "    +++++++\n",
    "    '''\n",
    "   \n",
    "\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db=client['reviews']\n",
    "    collection=db['user_reviews']\n",
    "    \n",
    "\n",
    "\n",
    "    documents = [x for x in db['user_reviews'].find().limit(find_lim)]\n",
    "    client.close()\n",
    "    all_revs = []\n",
    "    for idx, users in enumerate(documents):\n",
    "        try:\n",
    "            user = documents[idx]['userid']\n",
    "        except:\n",
    "            continue\n",
    "       \n",
    "        review_list = documents[idx]['reviews']\n",
    "        if len(review_list) ==0: #some users have not added any books to their goodreads profile\n",
    "            sub_rev = [None, None, None, None, None, user, None, None, None]\n",
    "            all_revs.append(sub_rev)\n",
    "        else:\n",
    "            for review in review_list:\n",
    "                soup = BeautifulSoup(review, 'html.parser')\n",
    "                title = soup.find_all(class_ = re.compile('title'))[0].text.split('\\n')[1].strip()\n",
    "                try:\n",
    "                    pages = int(soup.find_all(class_ =re.compile('num_pages'))[0].text.split()[2])\n",
    "                except:#Some works are infinite scroll documents without pages. \n",
    "                    pages = None\n",
    "                try:\n",
    "                    isbn = soup.find_all(class_ = re.compile('isbn13'))[0].text.strip('isbn13').strip()\n",
    "                except: #some works are not given ISBN\n",
    "                    isbn = None \n",
    "                book_type = soup.find_all(class_ = re.compile('format'))[0].text.split('\\n')[0].strip()\n",
    "                author = soup.find_all(class_ = re.compile('author'))[0].text.strip('author ').split('\\n')[0]\n",
    "                av_rate = float(soup.find_all(class_ =re.compile('avg_rating'))[0].text.split()[2])\n",
    "                num_rate = int(soup.find_all(class_ =re.compile('num_ratings'))[0].text.split()[2].replace(',',''))\n",
    "                user_rating = str_to_rate(soup.find_all(class_ =re.compile('field rating'))[0].text.split())\n",
    "                \n",
    "                sub_rev = [title,author, isbn, book_type, pages, user, user_rating, num_rate, av_rate]\n",
    "                all_revs.append(sub_rev)\n",
    "    return all_revs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
